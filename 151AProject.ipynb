{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split, cross_validate, cross_val_score\n",
    "from sklearn.metrics import mean_squared_error, classification_report\n",
    "from sklearn.linear_model import LinearRegression, LogisticRegression, Lasso, LassoCV, Ridge, RidgeCV"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preparing Datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "filepath = \"C:\\\\Users\\\\najiy\\\\OneDrive\\\\Desktop\\\\BerkeleyStuff\\\\Semesters\\\\Fall2020\\\\Stat151A\\\\Assignments\\\\Project\\\\hot-100\\\\\"\n",
    "\n",
    "#Billboard Hot 100 data\n",
    "hot_weekly = pd.read_csv(filepath+\"HotWeekly.csv\")\n",
    "\n",
    "#Spotify audio features data\n",
    "hot_features = pd.read_excel(filepath+\"Features.xlsx\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#convert WeekID's to datetime which will be handy for cleaning\n",
    "hot_weekly['WeekID'] = pd.to_datetime(hot_weekly['WeekID'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#hot_weekly has lots of repetition for many songs, we only really are interested in aggregate information. \n",
    "#we'll do a left join keeping final weekly appearances and matching it to it's appearance in hot_features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Add entry month and year columns\n",
    "hot_weekly['entry_month'] = hot_weekly['WeekID'].dt.month_name()\n",
    "hot_weekly['year'] = hot_weekly['WeekID'].dt.year"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "hot_weekly.sort_values(by = 'WeekID', inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Some songs reappear in the chart years after first entries\n",
    "#We are interested in aggregate information for songs first time entry alone\n",
    "\n",
    "#\n",
    "hot_weekly = hot_weekly.merge(hot_weekly.groupby('SongID')[['year']].first().reset_index(),\n",
    "                 how = 'left',\n",
    "                 left_on = 'SongID',\n",
    "                 right_on = 'SongID')\n",
    "\n",
    "hot_weekly = hot_weekly[~(abs(hot_weekly['year_x'] - hot_weekly['year_y']) > 4)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "#final weekly data with relevant columns\n",
    "hot_weekly_final = hot_weekly.sort_values(\n",
    "    by='WeekID',\n",
    "    ascending = False).groupby('SongID')[['Peak Position',\n",
    "                                          'Week Position',\n",
    "                                          'Weeks on Chart',\n",
    "                                          'entry_month',\n",
    "                                          'year_y',\n",
    "                                          'WeekID']].agg({\n",
    "    'Week Position': lambda x: x.iloc[-1],\n",
    "    'Peak Position': lambda x: x.iloc[0],\n",
    "    'Weeks on Chart': lambda x: x.iloc[0],\n",
    "    'entry_month': lambda x: x.iloc[-1], \n",
    "    'year_y' : lambda x: x.iloc[0],\n",
    "    'WeekID' : lambda x: x.iloc[-1]}).reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(True, False)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(hot_features['SongID'].value_counts().max() > 1, \n",
    " hot_weekly_final['SongID'].value_counts().max() > 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#Some songs in features datasets also appear twice so lets clean that up as well\n",
    "hot_features_clean = hot_features.groupby('SongID').first().reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(False, False)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(hot_features_clean['SongID'].value_counts().max() > 1, \n",
    " hot_weekly_final['SongID'].value_counts().max() > 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "hot_weekly_final.sort_values(by = ['year_y','WeekID'], inplace=True)\n",
    "\n",
    "#Additional field for aggregate songs count at the time of a songs release in the given year.\n",
    "hot_weekly_final['yr_chart_ct'] =hot_weekly_final.groupby('year_y').cumcount()+1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "#finally we have clean datasets so we can go ahead and join\n",
    "hot_merged = hot_weekly_final.merge(\n",
    "    hot_features_clean, \n",
    "    how = \"left\", \n",
    "    left_on = \"SongID\", \n",
    "    right_on = \"SongID\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Many records have a bunch of missing values, and basically that's just what it is, missing information\n",
    "#We need every information to do our analysis here and since removing songs doesnt really affect our goals we'll just\n",
    "#Drop all records with missing values\n",
    "#hot_merged.dropna(inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "#some of the names are kinda long, also inconsistent use of capitlizations so lets clean that up too\n",
    "hot_merged.rename({'SongID' : 'songid',\n",
    "                   'WeekID' : 'entry_weekid',\n",
    "                   'Week Position': 'entry_position',\n",
    "                   'Week Position2': 'avg_pos',\n",
    "                   'Peak Position' : 'peak',\n",
    "                   'Weeks on Chart': 'weeks',\n",
    "                   'Performer' : 'performer',\n",
    "                   'Song':'song',\n",
    "                   'spotify_genre' : 'st_genre',\n",
    "                   'spotify_track_album':'st_album',\n",
    "                   'spotify_track_explicit': 'st_explicit',\n",
    "                   'spotify_track_duration_ms':'st_duration_ms',\n",
    "                   'spotify_track_popularity':'st_popularity' }, axis = 1, inplace = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Further Data Cleaning/Mod"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 204,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>entry_position</th>\n",
       "      <th>peak</th>\n",
       "      <th>weeks</th>\n",
       "      <th>year_y</th>\n",
       "      <th>yr_chart_ct</th>\n",
       "      <th>st_explicit</th>\n",
       "      <th>st_duration_ms</th>\n",
       "      <th>st_popularity</th>\n",
       "      <th>danceability</th>\n",
       "      <th>energy</th>\n",
       "      <th>key</th>\n",
       "      <th>loudness</th>\n",
       "      <th>mode</th>\n",
       "      <th>speechiness</th>\n",
       "      <th>acousticness</th>\n",
       "      <th>instrumentalness</th>\n",
       "      <th>liveness</th>\n",
       "      <th>valence</th>\n",
       "      <th>tempo</th>\n",
       "      <th>time_signature</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>28474.000000</td>\n",
       "      <td>28474.000000</td>\n",
       "      <td>28474.00000</td>\n",
       "      <td>28474.000000</td>\n",
       "      <td>28474.000000</td>\n",
       "      <td>23634.000000</td>\n",
       "      <td>2.363400e+04</td>\n",
       "      <td>23634.000000</td>\n",
       "      <td>23572.000000</td>\n",
       "      <td>23572.000000</td>\n",
       "      <td>23572.000000</td>\n",
       "      <td>23572.000000</td>\n",
       "      <td>23572.000000</td>\n",
       "      <td>23572.000000</td>\n",
       "      <td>23572.000000</td>\n",
       "      <td>23572.000000</td>\n",
       "      <td>23572.000000</td>\n",
       "      <td>23572.000000</td>\n",
       "      <td>23572.000000</td>\n",
       "      <td>23572.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>81.012748</td>\n",
       "      <td>47.189015</td>\n",
       "      <td>11.23748</td>\n",
       "      <td>1985.276709</td>\n",
       "      <td>248.060441</td>\n",
       "      <td>0.108953</td>\n",
       "      <td>2.214737e+05</td>\n",
       "      <td>40.360921</td>\n",
       "      <td>0.597956</td>\n",
       "      <td>0.618061</td>\n",
       "      <td>5.242491</td>\n",
       "      <td>-8.723276</td>\n",
       "      <td>0.729722</td>\n",
       "      <td>0.071608</td>\n",
       "      <td>0.296239</td>\n",
       "      <td>0.033375</td>\n",
       "      <td>0.192719</td>\n",
       "      <td>0.605555</td>\n",
       "      <td>120.203157</td>\n",
       "      <td>3.930553</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>18.338973</td>\n",
       "      <td>30.970541</td>\n",
       "      <td>8.13157</td>\n",
       "      <td>18.659866</td>\n",
       "      <td>163.086675</td>\n",
       "      <td>0.311587</td>\n",
       "      <td>6.819471e+04</td>\n",
       "      <td>22.095835</td>\n",
       "      <td>0.152898</td>\n",
       "      <td>0.200157</td>\n",
       "      <td>3.560444</td>\n",
       "      <td>3.613220</td>\n",
       "      <td>0.444113</td>\n",
       "      <td>0.080373</td>\n",
       "      <td>0.283015</td>\n",
       "      <td>0.138096</td>\n",
       "      <td>0.159914</td>\n",
       "      <td>0.238231</td>\n",
       "      <td>27.988539</td>\n",
       "      <td>0.320639</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.00000</td>\n",
       "      <td>1958.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>2.968800e+04</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000581</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-28.030000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000003</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.009670</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>75.000000</td>\n",
       "      <td>18.000000</td>\n",
       "      <td>5.00000</td>\n",
       "      <td>1969.000000</td>\n",
       "      <td>115.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.754260e+05</td>\n",
       "      <td>22.000000</td>\n",
       "      <td>0.497000</td>\n",
       "      <td>0.473000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>-11.115250</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.032000</td>\n",
       "      <td>0.046775</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.090500</td>\n",
       "      <td>0.421000</td>\n",
       "      <td>99.106250</td>\n",
       "      <td>4.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>86.000000</td>\n",
       "      <td>47.000000</td>\n",
       "      <td>10.00000</td>\n",
       "      <td>1982.000000</td>\n",
       "      <td>230.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>2.157595e+05</td>\n",
       "      <td>42.000000</td>\n",
       "      <td>0.606000</td>\n",
       "      <td>0.634000</td>\n",
       "      <td>5.000000</td>\n",
       "      <td>-8.300000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.040900</td>\n",
       "      <td>0.197000</td>\n",
       "      <td>0.000005</td>\n",
       "      <td>0.131000</td>\n",
       "      <td>0.627000</td>\n",
       "      <td>118.779500</td>\n",
       "      <td>4.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>94.000000</td>\n",
       "      <td>75.000000</td>\n",
       "      <td>16.00000</td>\n",
       "      <td>2002.000000</td>\n",
       "      <td>351.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>2.543060e+05</td>\n",
       "      <td>58.000000</td>\n",
       "      <td>0.706000</td>\n",
       "      <td>0.779000</td>\n",
       "      <td>8.000000</td>\n",
       "      <td>-5.901000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.066500</td>\n",
       "      <td>0.514000</td>\n",
       "      <td>0.000510</td>\n",
       "      <td>0.250000</td>\n",
       "      <td>0.805000</td>\n",
       "      <td>136.102500</td>\n",
       "      <td>4.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>100.000000</td>\n",
       "      <td>100.000000</td>\n",
       "      <td>87.00000</td>\n",
       "      <td>2019.000000</td>\n",
       "      <td>740.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>3.079157e+06</td>\n",
       "      <td>100.000000</td>\n",
       "      <td>0.988000</td>\n",
       "      <td>0.997000</td>\n",
       "      <td>11.000000</td>\n",
       "      <td>2.291000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.951000</td>\n",
       "      <td>0.991000</td>\n",
       "      <td>0.982000</td>\n",
       "      <td>0.999000</td>\n",
       "      <td>0.991000</td>\n",
       "      <td>241.009000</td>\n",
       "      <td>5.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       entry_position          peak        weeks        year_y   yr_chart_ct  \\\n",
       "count    28474.000000  28474.000000  28474.00000  28474.000000  28474.000000   \n",
       "mean        81.012748     47.189015     11.23748   1985.276709    248.060441   \n",
       "std         18.338973     30.970541      8.13157     18.659866    163.086675   \n",
       "min          1.000000      1.000000      1.00000   1958.000000      1.000000   \n",
       "25%         75.000000     18.000000      5.00000   1969.000000    115.000000   \n",
       "50%         86.000000     47.000000     10.00000   1982.000000    230.000000   \n",
       "75%         94.000000     75.000000     16.00000   2002.000000    351.000000   \n",
       "max        100.000000    100.000000     87.00000   2019.000000    740.000000   \n",
       "\n",
       "        st_explicit  st_duration_ms  st_popularity  danceability  \\\n",
       "count  23634.000000    2.363400e+04   23634.000000  23572.000000   \n",
       "mean       0.108953    2.214737e+05      40.360921      0.597956   \n",
       "std        0.311587    6.819471e+04      22.095835      0.152898   \n",
       "min        0.000000    2.968800e+04       0.000000      0.000000   \n",
       "25%        0.000000    1.754260e+05      22.000000      0.497000   \n",
       "50%        0.000000    2.157595e+05      42.000000      0.606000   \n",
       "75%        0.000000    2.543060e+05      58.000000      0.706000   \n",
       "max        1.000000    3.079157e+06     100.000000      0.988000   \n",
       "\n",
       "             energy           key      loudness          mode   speechiness  \\\n",
       "count  23572.000000  23572.000000  23572.000000  23572.000000  23572.000000   \n",
       "mean       0.618061      5.242491     -8.723276      0.729722      0.071608   \n",
       "std        0.200157      3.560444      3.613220      0.444113      0.080373   \n",
       "min        0.000581      0.000000    -28.030000      0.000000      0.000000   \n",
       "25%        0.473000      2.000000    -11.115250      0.000000      0.032000   \n",
       "50%        0.634000      5.000000     -8.300000      1.000000      0.040900   \n",
       "75%        0.779000      8.000000     -5.901000      1.000000      0.066500   \n",
       "max        0.997000     11.000000      2.291000      1.000000      0.951000   \n",
       "\n",
       "       acousticness  instrumentalness      liveness       valence  \\\n",
       "count  23572.000000      23572.000000  23572.000000  23572.000000   \n",
       "mean       0.296239          0.033375      0.192719      0.605555   \n",
       "std        0.283015          0.138096      0.159914      0.238231   \n",
       "min        0.000003          0.000000      0.009670      0.000000   \n",
       "25%        0.046775          0.000000      0.090500      0.421000   \n",
       "50%        0.197000          0.000005      0.131000      0.627000   \n",
       "75%        0.514000          0.000510      0.250000      0.805000   \n",
       "max        0.991000          0.982000      0.999000      0.991000   \n",
       "\n",
       "              tempo  time_signature  \n",
       "count  23572.000000    23572.000000  \n",
       "mean     120.203157        3.930553  \n",
       "std       27.988539        0.320639  \n",
       "min        0.000000        0.000000  \n",
       "25%       99.106250        4.000000  \n",
       "50%      118.779500        4.000000  \n",
       "75%      136.102500        4.000000  \n",
       "max      241.009000        5.000000  "
      ]
     },
     "execution_count": 204,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hot_merged.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 205,
   "metadata": {},
   "outputs": [],
   "source": [
    "pattern = r\"[^a-zA-Z]\"\n",
    "hot_merged['st_genre2'] = hot_merged['st_genre'].str.lower().str.replace(pattern, \"\")\n",
    "hot_merged['performer'] = hot_merged['performer'].str.lower().str.replace(pattern, \"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 206,
   "metadata": {},
   "outputs": [],
   "source": [
    "pattern = r\"[^a-zA-Z,]\"\n",
    "hot_merged['st_genre'] = hot_merged['st_genre'].str.lower().str.replace(\n",
    "    pattern, \"\").str.split(\",\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 207,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Add a field for select song genres.\n",
    "gnr_dict = {'pop':r'pop',\n",
    " 'country':r\"country|folk\",\n",
    " 'hiphop':r'rap|hiphop|trap',\n",
    " 'r&b':r'rb|rhythmandblues',\n",
    " 'rock':r'rock|rockandroll',\n",
    " 'dance':'disco|dance'}\n",
    "\n",
    "for key in gnr_dict:\n",
    "    hot_merged[key] = hot_merged['st_genre2'].str.contains(gnr_dict[key])*1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 208,
   "metadata": {},
   "outputs": [],
   "source": [
    "def genre_match(dflist, gdict):\n",
    "    if type(dflist) is list:\n",
    "        for genre in dflist:\n",
    "            for key in gdict:\n",
    "                if genre in gdict[key]:\n",
    "                    return key\n",
    "    return 'other'\n",
    "\n",
    "#Different genre field approach\n",
    "gnr_dict2 = {'pop':['pop', 'dancepop', 'kpop'],\n",
    " 'country':['country', 'folk'],\n",
    " 'hiphop':['rap','hiphop','trap'],\n",
    " 'r&b':['rb' 'rhythmandblues'],\n",
    " 'rock':['rock', 'rockandroll'],\n",
    " 'dance':['disco', 'dance']}\n",
    "\n",
    "hot_merged['genre'] = hot_merged['st_genre'].apply(lambda x: genre_match(x, gnr_dict2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 209,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Add a field for weather a song had a featured artist\n",
    "pattern = r\"(featuring\\w+)\"\n",
    "hot_merged['feat_artist'] = hot_merged['performer'].str.extract(\n",
    "    pattern).replace({np.nan:'None'})[0]\n",
    "hot_merged['performer'] = hot_merged['performer'].str.replace(pattern,\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 210,
   "metadata": {},
   "outputs": [],
   "source": [
    "hot_merged['feat_artist'] = hot_merged['feat_artist'].str.replace(r\"featuring\", \"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 211,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Add a field for count of previous charted songs by an artist\n",
    "hot_merged['art_prev_100_ct'] = hot_merged.groupby('performer').cumcount()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 212,
   "metadata": {},
   "outputs": [],
   "source": [
    "hot_merged['has_feat'] = ~(hot_merged['feat_artist'] == 'None')+0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 214,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Export current dataframe\n",
    "hot_merged.drop(['spotify_track_id',\n",
    "                 'song',\n",
    "                 'spotify_track_preview_url',\n",
    "                 'st_album',\n",
    "                 'st_genre',\n",
    "                 'st_genre2'], axis=1).to_csv(filepath+\"hot_merged_full.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Regression Practice"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {},
   "outputs": [],
   "source": [
    "for col in hot_merged.columns.values:\n",
    "    if type(hot_merged[col][0]) == np.float64:\n",
    "        hot_merged[col] = hot_merged[col].round(decimals=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rmse(y_obs, y_pred):\n",
    "    return np.sqrt(np.mean((y_obs-y_pred)**2))\n",
    "\n",
    "def oh_encode_no_redundant(data):\n",
    "    \"\"\"\n",
    "    Return the one-hot encoded dataframe of our input data, removing redundancies.\n",
    "    \n",
    "    Parameters\n",
    "    -----------\n",
    "    data: a dataframe that may include non-numerical features\n",
    "    \n",
    "    Returns\n",
    "    -----------\n",
    "    A one-hot encoded dataframe that only contains numeric features without any redundancies.\n",
    "    \n",
    "    \"\"\"\n",
    "    encoded_data = pd.get_dummies(data)\n",
    "    indices = np.where(np.corrcoef(encoded_data.T)[0] > 0.6)[0]\n",
    "    if indices.size > 1:\n",
    "        redundant_cols = encoded_data.columns[indices[1:]]\n",
    "        return encoded_data.drop(redundant_cols, axis=1)\n",
    "    else:\n",
    "        return encoded_data\n",
    "    \n",
    "def guided_feature_eng(data, preprocess=None):\n",
    "    data = oh_encode_no_redundant(data.copy())\n",
    "    if preprocess is not None:\n",
    "        return preprocess.fit_transform(data)\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [],
   "source": [
    "features_id = ['year','entry_position', 'st_duration_ms', 'danceability', 'energy',\n",
    "               'key','loudness', 'mode', 'speechiness','acousticness', 'instrumentalness',\n",
    "               'liveness', 'valence', 'tempo','st_explicit', 'has_feat_artist', 'genre']\n",
    "\n",
    "        \n",
    "#seperate into training and testing data   \n",
    "tr_response = hot_merged.query(\"year < 2010\").dropna()['weeks']\n",
    "tr_features = hot_merged.query(\"year < 2010\").dropna().set_index('songid')[features_id]\n",
    "\n",
    "te_response = hot_merged.query(\"year >= 2010\").dropna()['weeks']\n",
    "te_features = hot_merged.query(\"year >= 2010\").dropna().set_index('songid')[features_id]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import SCORERS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.23389646661308505"
      ]
     },
     "execution_count": 107,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#first model fit\n",
    "lin_model = LinearRegression()\n",
    "tr_feat_basic = guided_feature_eng(tr_features)\n",
    "\n",
    "\n",
    "lin_model.fit(tr_feat_basic, tr_response);\n",
    "lin_model.score(tr_feat_basic, tr_response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'fit_time': array([0.02500582, 0.00700164, 0.00900221, 0.00700188, 0.0070014 ]),\n",
       " 'score_time': array([0.00100088, 0.00100017, 0.00099993, 0.00100017, 0.        ]),\n",
       " 'test_score': array([-6.56650328, -6.74207243, -6.31628242, -6.35048426, -6.60436557])}"
      ]
     },
     "execution_count": 108,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#training set performance test with cross validation\n",
    "cross_validate(LinearRegression(), tr_feat_basic, tr_response, scoring='neg_root_mean_squared_error')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exploring Improvements"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import PolynomialFeatures"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [],
   "source": [
    "inter_poly = PolynomialFeatures(interaction_only=True)\n",
    "poly_pw = PolynomialFeatures(degree=2)\n",
    "lasso_model = Lasso()\n",
    "ridge_model = Ridge()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [],
   "source": [
    "tr_feat_inter = guided_feature_eng(tr_features, inter_poly)\n",
    "tr_feat_quad = guided_feature_eng(tr_features, poly_pw)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(6.556574733277452, 6.501736824045065)"
      ]
     },
     "execution_count": 112,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#standard and regularized models training data fit and error check\n",
    "lin_model.fit(tr_feat_basic, tr_response)\n",
    "lasso_model.fit(tr_feat_basic, tr_response)\n",
    "ridge_model.fit(tr_feat_basic, tr_response)\n",
    "\n",
    "(rmse(tr_response, lasso_model.predict(tr_feat_basic)),\n",
    " rmse(tr_response, ridge_model.predict(tr_feat_basic)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(6.3585297351207375, 0.26727235980666697)"
      ]
     },
     "execution_count": 113,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lin_model.fit(tr_feat_quad, tr_response);\n",
    "(rmse(tr_response, lin_model.predict(tr_feat_quad)), lin_model.score(tr_feat_quad, tr_response))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(6.399816445012737, 0.2577260874893883)"
      ]
     },
     "execution_count": 115,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lin_model.fit(tr_feat_inter, tr_response);\n",
    "(rmse(tr_response, lin_model.predict(tr_feat_inter)) , lin_model.score(tr_feat_inter, tr_response))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'fit_time': array([0.11953259, 0.18204093, 0.08101797, 0.17103887, 0.08001828]),\n",
       " 'score_time': array([0.00200033, 0.00100255, 0.00200248, 0.00200295, 0.00200009]),\n",
       " 'test_score': array([0.20741118, 0.21399939, 0.23640651, 0.18723357, 0.24502156])}"
      ]
     },
     "execution_count": 117,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#check performance of features w/interaction using cross validation\n",
    "cross_validate(LinearRegression(),\n",
    "               tr_feat_inter,\n",
    "               tr_response,\n",
    "               scoring='explained_variance')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'fit_time': array([0.15103412, 0.1330297 , 0.11202526, 0.15803552, 0.11202502]),\n",
       " 'score_time': array([0.00200319, 0.00200057, 0.00200033, 0.00200057, 0.00100017]),\n",
       " 'test_score': array([-3.90638835, -3.89530114, -3.6192317 , -3.58797865, -3.90352306])}"
      ]
     },
     "execution_count": 103,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#check performance of quadratic features using cross validation\n",
    "cross_validate(LinearRegression(),\n",
    "               tr_feat_quad,\n",
    "               tr_response,\n",
    "               scoring='neg_root_mean_squared_error')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Logistic Regression V2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Using initial hot_merged dataset, will add a rank_level variable to be used as a 10 class categorical variable\n",
    "rank_dict = {}\n",
    "rank_dict[\"Top 10\"] = np.arange(start= 1, stop=11, step=1)\n",
    "rank_dict[\"Not Top 10\"] = np.arange(start = 11, stop=101, step= 1)\n",
    "\n",
    "def rank_match(rank):\n",
    "    for key in rank_dict.keys():\n",
    "        if rank in rank_dict[key]:\n",
    "            return key\n",
    "        \n",
    "hot_merged['rank_level'] = hot_merged['peak'].apply(lambda x: rank_match(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "balanced_dat = pd.concat([hot_merged.query(\"peak > 10\").sample(n=4000), \n",
    "           hot_merged.query(\"peak <= 10\").sample(n=4000)])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "features_id = ['entry_position', 'artist_prev_hot100_songs', 'st_duration_ms', 'danceability', 'energy',\n",
    "               'key','loudness', 'mode', 'speechiness','acousticness', 'instrumentalness',\n",
    "               'liveness', 'valence', 'tempo','st_explicit', 'has_feat_artist', \n",
    "               'pop','country','rock','hiphop','r&b','dance']\n",
    "\n",
    "        \n",
    "#seperate into training and testing data   \n",
    "tr_response = balanced_dat.query(\"year < 2010\").dropna()['rank_level']\n",
    "tr_features = balanced_dat.query(\"year < 2010\").dropna().set_index('songid')[features_id]\n",
    "\n",
    "te_response = balanced_dat.query(\"year >= 2010\").dropna()['rank_level']\n",
    "te_features = balanced_dat.query(\"year >= 2010\").dropna().set_index('songid')[features_id]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "tr_features['st_duration_ms'] = (tr_features['st_duration_ms']- np.mean(tr_features['st_duration_ms']))/np.std(tr_features['st_duration_ms'] )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "log_model = LogisticRegression(fit_intercept = True, solver = 'lbfgs', max_iter=1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "  Not Top 10       0.63      0.66      0.65      1600\n",
      "      Top 10       0.69      0.67      0.68      1856\n",
      "\n",
      "    accuracy                           0.67      3456\n",
      "   macro avg       0.66      0.67      0.66      3456\n",
      "weighted avg       0.67      0.67      0.67      3456\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\najiy\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:940: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n"
     ]
    }
   ],
   "source": [
    "log_model.fit(guided_feature_eng(tr_features), tr_response)\n",
    "print(classification_report(tr_response, log_model.predict(guided_feature_eng(tr_features))))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
